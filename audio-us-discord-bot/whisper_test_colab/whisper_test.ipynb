{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLZi1Zg4M_Vl"
      },
      "outputs": [],
      "source": [
        "pip install faster-whisper fastapi uvicorn pyngrok numpy nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lkKEsJaz-whf"
      },
      "outputs": [],
      "source": [
        "!rm -rf ~/.cache/huggingface/hub/models--Systran--faster-whisper-large-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KXQ0dD9GLsCk"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "import numpy as np\n",
        "import librosa\n",
        "from functools import lru_cache\n",
        "import time\n",
        "import logging\n",
        "\n",
        "import io\n",
        "import soundfile as sf\n",
        "import math\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@lru_cache(10**6)\n",
        "def load_audio(fname):\n",
        "    a, _ = librosa.load(fname, sr=16000, dtype=np.float32)\n",
        "    return a\n",
        "\n",
        "def load_audio_chunk(fname, beg, end):\n",
        "    audio = load_audio(fname)\n",
        "    beg_s = int(beg*16000)\n",
        "    end_s = int(end*16000)\n",
        "    return audio[beg_s:end_s]\n",
        "\n",
        "\n",
        "# Whisper backend\n",
        "\n",
        "class ASRBase:\n",
        "\n",
        "    sep = \" \"  # Join transcribe words with this character (\" \" for whisper_timestamped)\n",
        "\n",
        "    def __init__(self, lan, modelsize=None, cache_dir=None, model_dir=None, logfile=sys.stderr):\n",
        "        self.logfile = logfile\n",
        "\n",
        "        self.transcribe_kargs = {}\n",
        "        if lan == \"auto\":\n",
        "            self.original_language = None\n",
        "        else:\n",
        "            self.original_language = lan\n",
        "\n",
        "        self.model = self.load_model(modelsize, cache_dir, model_dir)\n",
        "\n",
        "    def load_model(self, modelsize, cache_dir):\n",
        "        raise NotImplemented(\"must be implemented in the child class\")\n",
        "\n",
        "    def transcribe(self, audio, init_prompt=\"\"):\n",
        "        raise NotImplemented(\"must be implemented in the child class\")\n",
        "\n",
        "    def use_vad(self):\n",
        "        raise NotImplemented(\"must be implemented in the child class\")\n",
        "\n",
        "class FasterWhisperASR(ASRBase):\n",
        "    \"\"\"Uses faster-whisper library as the backend. Works much faster, appx 4-times (in offline mode). For GPU, it requires installation with a specific CUDNN version.\n",
        "    \"\"\"\n",
        "\n",
        "    sep = \"\"\n",
        "\n",
        "    def load_model(self, modelsize=None, cache_dir=None, model_dir=None):\n",
        "        from faster_whisper import WhisperModel\n",
        "#        logging.getLogger(\"faster_whisper\").setLevel(logger.level)\n",
        "        if model_dir is not None:\n",
        "            logger.debug(f\"Loading whisper model from model_dir {model_dir}. modelsize and cache_dir parameters are not used.\")\n",
        "            model_size_or_path = model_dir\n",
        "        elif modelsize is not None:\n",
        "            model_size_or_path = modelsize\n",
        "        else:\n",
        "            raise ValueError(\"modelsize or model_dir parameter must be set\")\n",
        "\n",
        "\n",
        "        # this worked fast and reliably on NVIDIA L40\n",
        "        # model = WhisperModel(model_size_or_path, device=\"cuda\", compute_type=\"float16\", download_root=cache_dir)\n",
        "\n",
        "        # or run on GPU with INT8\n",
        "        # tested: the transcripts were different, probably worse than with FP16, and it was slightly (appx 20%) slower\n",
        "        #model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
        "\n",
        "        # or run on CPU with INT8\n",
        "        # tested: works, but slow, appx 10-times than cuda FP16\n",
        "        model = WhisperModel(modelsize, device=\"cpu\", compute_type=\"int8\") #, download_root=\"faster-disk-cache-dir/\")\n",
        "        return model\n",
        "\n",
        "    def transcribe(self, audio, init_prompt=\"\"):\n",
        "\n",
        "        # tested: beam_size=5 is faster and better than 1 (on one 200 second document from En ESIC, min chunk 0.01)\n",
        "        segments, info = self.model.transcribe(audio, language=self.original_language, initial_prompt=init_prompt, beam_size=5, word_timestamps=True, condition_on_previous_text=True, **self.transcribe_kargs)\n",
        "        #print(info)  # info contains language detection result\n",
        "\n",
        "        return list(segments)\n",
        "\n",
        "    def ts_words(self, segments):\n",
        "        o = []\n",
        "        for segment in segments:\n",
        "            for word in segment.words:\n",
        "                if segment.no_speech_prob > 0.9:\n",
        "                    continue\n",
        "                # not stripping the spaces -- should not be merged with them!\n",
        "                w = word.word\n",
        "                t = (word.start, word.end, w)\n",
        "                o.append(t)\n",
        "        return o\n",
        "\n",
        "    def segments_end_ts(self, res):\n",
        "        return [s.end for s in res]\n",
        "\n",
        "    def use_vad(self):\n",
        "        self.transcribe_kargs[\"vad_filter\"] = True\n",
        "\n",
        "    def set_translate_task(self):\n",
        "        self.transcribe_kargs[\"task\"] = \"translate\"\n",
        "\n",
        "\n",
        "\n",
        "class HypothesisBuffer:\n",
        "\n",
        "    def __init__(self, logfile=sys.stderr):\n",
        "        self.commited_in_buffer = []\n",
        "        self.buffer = []\n",
        "        self.new = []\n",
        "\n",
        "        self.last_commited_time = 0\n",
        "        self.last_commited_word = None\n",
        "\n",
        "        self.logfile = logfile\n",
        "\n",
        "    def insert(self, new, offset):\n",
        "        # Compare self.commited_in_buffer and new. It inserts only the words in new that extend the commited_in_buffer\n",
        "        new = [(a + offset, b + offset, t) for a, b, t in new]\n",
        "        self.new = [(a, b, t) for a, b, t in new if a > self.last_commited_time - 0.1]\n",
        "\n",
        "        if len(self.new) >= 1:\n",
        "            a, b, t = self.new[0]\n",
        "            if abs(a - self.last_commited_time) < 1:\n",
        "                if self.commited_in_buffer:\n",
        "                    # Search for 1, 2, ..., 5 consecutive words (n-grams) that are identical in commited and new\n",
        "                    cn = len(self.commited_in_buffer)\n",
        "                    nn = len(self.new)\n",
        "                    for i in range(1, min(min(cn, nn), 5) + 1):  # 5 is the maximum\n",
        "                        c = \" \".join([self.commited_in_buffer[-j][2] for j in range(1, i + 1)][::-1])\n",
        "                        tail = \" \".join(self.new[j-1][2] for j in range(1, i + 1))\n",
        "                        if c == tail:\n",
        "                            words = []\n",
        "                            for j in range(i):\n",
        "                                words.append(repr(self.new.pop(0)))\n",
        "                            words_msg = \" \".join(words)\n",
        "                            logger.debug(f\"removing last {i} words: {words_msg}\")\n",
        "                            break\n",
        "\n",
        "    def flush(self):\n",
        "        # Returns commited chunk = the longest common prefix of 2 last inserts\n",
        "        commit = []\n",
        "        while self.new:\n",
        "            na, nb, nt = self.new[0]\n",
        "\n",
        "            if len(self.buffer) == 0:\n",
        "                break\n",
        "\n",
        "            if nt == self.buffer[0][2]:\n",
        "                commit.append((na, nb, nt))\n",
        "                self.last_commited_word = nt\n",
        "                self.last_commited_time = nb\n",
        "                self.buffer.pop(0)\n",
        "                self.new.pop(0)\n",
        "            else:\n",
        "                break\n",
        "        self.buffer = self.new\n",
        "        self.new = []\n",
        "        self.commited_in_buffer.extend(commit)\n",
        "        return commit\n",
        "\n",
        "    def pop_commited(self, time):\n",
        "        while self.commited_in_buffer and self.commited_in_buffer[0][1] <= time:\n",
        "            self.commited_in_buffer.pop(0)\n",
        "\n",
        "    def complete(self):\n",
        "        return self.buffer\n",
        "\n",
        "class OnlineASRProcessor:\n",
        "\n",
        "    SAMPLING_RATE = 16000\n",
        "\n",
        "    def __init__(self, asr, tokenizer=None, buffer_trimming=(\"segment\", 15), logfile=sys.stderr):\n",
        "        \"\"\"asr: WhisperASR object\n",
        "        tokenizer: sentence tokenizer object for the target language. Must have a method *split* that behaves like the one of MosesTokenizer. It can be None, if \"segment\" buffer trimming option is used, then tokenizer is not used at all.\n",
        "        (\"segment\", 15)\n",
        "        buffer_trimming: a pair of (option, seconds), where option is either \"sentence\" or \"segment\", and seconds is a number. Buffer is trimmed if it is longer than \"seconds\" threshold. Default is the most recommended option.\n",
        "        logfile: where to store the log.\n",
        "        \"\"\"\n",
        "        self.asr = asr\n",
        "        self.tokenizer = tokenizer\n",
        "        self.logfile = logfile\n",
        "\n",
        "        self.init()\n",
        "\n",
        "        self.buffer_trimming_way, self.buffer_trimming_sec = buffer_trimming\n",
        "\n",
        "    def init(self, offset=None):\n",
        "        \"\"\"run this when starting or restarting processing\"\"\"\n",
        "        self.audio_buffer = np.array([],dtype=np.float32)\n",
        "        self.transcript_buffer = HypothesisBuffer(logfile=self.logfile)\n",
        "        self.buffer_time_offset = 0\n",
        "        if offset is not None:\n",
        "            self.buffer_time_offset = offset\n",
        "        self.transcript_buffer.last_commited_time = self.buffer_time_offset\n",
        "        self.commited = []\n",
        "\n",
        "    def insert_audio_chunk(self, audio):\n",
        "        self.audio_buffer = np.append(self.audio_buffer, audio)\n",
        "        sf.write(f\"debug_audio_{len(self.audio_buffer)}.wav\", self.audio_buffer, 16000)\n",
        "\n",
        "    def prompt(self):\n",
        "        \"\"\"Returns a tuple: (prompt, context), where \"prompt\" is a 200-character suffix of commited text that is inside of the scrolled away part of audio buffer.\n",
        "        \"context\" is the commited text that is inside the audio buffer. It is transcribed again and skipped. It is returned only for debugging and logging reasons.\n",
        "        \"\"\"\n",
        "        k = max(0,len(self.commited)-1)\n",
        "        while k > 0 and self.commited[k-1][1] > self.buffer_time_offset:\n",
        "            k -= 1\n",
        "\n",
        "        p = self.commited[:k]\n",
        "        p = [t for _,_,t in p]\n",
        "        prompt = []\n",
        "        l = 0\n",
        "        while p and l < 200:  # 200 characters prompt size\n",
        "            x = p.pop(-1)\n",
        "            l += len(x)+1\n",
        "            prompt.append(x)\n",
        "        non_prompt = self.commited[k:]\n",
        "        return self.asr.sep.join(prompt[::-1]), self.asr.sep.join(t for _,_,t in non_prompt)\n",
        "\n",
        "    def process_iter(self):\n",
        "        \"\"\"Runs on the current audio buffer.\n",
        "        Returns: a tuple (beg_timestamp, end_timestamp, \"text\"), or (None, None, \"\").\n",
        "        The non-emty text is confirmed (committed) partial transcript.\n",
        "        \"\"\"\n",
        "\n",
        "        prompt, non_prompt = self.prompt()\n",
        "        logger.debug(f\"PROMPT: {prompt}\")\n",
        "        logger.debug(f\"CONTEXT: {non_prompt}\")\n",
        "        logger.debug(f\"transcribing {len(self.audio_buffer)/self.SAMPLING_RATE:2.2f} seconds from {self.buffer_time_offset:2.2f}\")\n",
        "        res = self.asr.transcribe(self.audio_buffer, init_prompt=prompt)\n",
        "\n",
        "        # transform to [(beg,end,\"word1\"), ...]\n",
        "        tsw = self.asr.ts_words(res)\n",
        "\n",
        "        self.transcript_buffer.insert(tsw, self.buffer_time_offset)\n",
        "        o = self.transcript_buffer.flush()\n",
        "        self.commited.extend(o)\n",
        "        completed = self.to_flush(o)\n",
        "        logger.debug(f\">>>>COMPLETE NOW: {completed}\")\n",
        "        the_rest = self.to_flush(self.transcript_buffer.complete())\n",
        "        logger.debug(f\"INCOMPLETE: {the_rest}\")\n",
        "\n",
        "        # there is a newly confirmed text\n",
        "\n",
        "        if o and self.buffer_trimming_way == \"sentence\":  # trim the completed sentences\n",
        "            if len(self.audio_buffer)/self.SAMPLING_RATE > self.buffer_trimming_sec:  # longer than this\n",
        "                self.chunk_completed_sentence()\n",
        "\n",
        "\n",
        "        if self.buffer_trimming_way == \"segment\":\n",
        "            s = self.buffer_trimming_sec  # trim the completed segments longer than s,\n",
        "        else:\n",
        "            s = 30 # if the audio buffer is longer than 30s, trim it\n",
        "\n",
        "        if len(self.audio_buffer)/self.SAMPLING_RATE > s:\n",
        "            self.chunk_completed_segment(res)\n",
        "\n",
        "            # alternative: on any word\n",
        "            #l = self.buffer_time_offset + len(self.audio_buffer)/self.SAMPLING_RATE - 10\n",
        "            # let's find commited word that is less\n",
        "            #k = len(self.commited)-1\n",
        "            #while k>0 and self.commited[k][1] > l:\n",
        "            #    k -= 1\n",
        "            #t = self.commited[k][1]\n",
        "            logger.debug(\"chunking segment\")\n",
        "            #self.chunk_at(t)\n",
        "\n",
        "        logger.debug(f\"len of buffer now: {len(self.audio_buffer)/self.SAMPLING_RATE:2.2f}\")\n",
        "        return self.to_flush(o)\n",
        "\n",
        "    def chunk_completed_sentence(self):\n",
        "        if self.commited == []: return\n",
        "        logger.debug(self.commited)\n",
        "        sents = self.words_to_sentences(self.commited)\n",
        "        for s in sents:\n",
        "            logger.debug(f\"\\t\\tSENT: {s}\")\n",
        "        if len(sents) < 2:\n",
        "            return\n",
        "        while len(sents) > 2:\n",
        "            sents.pop(0)\n",
        "        # we will continue with audio processing at this timestamp\n",
        "        chunk_at = sents[-2][1]\n",
        "\n",
        "        logger.debug(f\"--- sentence chunked at {chunk_at:2.2f}\")\n",
        "        self.chunk_at(chunk_at)\n",
        "\n",
        "    def chunk_completed_segment(self, res):\n",
        "        if self.commited == []: return\n",
        "\n",
        "        ends = self.asr.segments_end_ts(res)\n",
        "\n",
        "        t = self.commited[-1][1]\n",
        "\n",
        "        if len(ends) > 1:\n",
        "\n",
        "            e = ends[-2]+self.buffer_time_offset\n",
        "            while len(ends) > 2 and e > t:\n",
        "                ends.pop(-1)\n",
        "                e = ends[-2]+self.buffer_time_offset\n",
        "            if e <= t:\n",
        "                logger.debug(f\"--- segment chunked at {e:2.2f}\")\n",
        "                self.chunk_at(e)\n",
        "            else:\n",
        "                logger.debug(f\"--- last segment not within commited area\")\n",
        "        else:\n",
        "            logger.debug(f\"--- not enough segments to chunk\")\n",
        "\n",
        "\n",
        "\n",
        "    def chunk_at(self, time):\n",
        "        \"\"\"trims the hypothesis and audio buffer at \"time\"\n",
        "        \"\"\"\n",
        "        self.transcript_buffer.pop_commited(time)\n",
        "        cut_seconds = time - self.buffer_time_offset\n",
        "        self.audio_buffer = self.audio_buffer[int(cut_seconds*self.SAMPLING_RATE):]\n",
        "        self.buffer_time_offset = time\n",
        "\n",
        "    def words_to_sentences(self, words):\n",
        "        \"\"\"Uses self.tokenizer for sentence segmentation of words.\n",
        "        Returns: [(beg,end,\"sentence 1\"),...]\n",
        "        \"\"\"\n",
        "\n",
        "        cwords = [w for w in words]\n",
        "        t = \" \".join(o[2] for o in cwords)\n",
        "        s = self.tokenizer.split(t)\n",
        "        out = []\n",
        "        while s:\n",
        "            beg = None\n",
        "            end = None\n",
        "            sent = s.pop(0).strip()\n",
        "            fsent = sent\n",
        "            while cwords:\n",
        "                b,e,w = cwords.pop(0)\n",
        "                w = w.strip()\n",
        "                if beg is None and sent.startswith(w):\n",
        "                    beg = b\n",
        "                elif end is None and sent == w:\n",
        "                    end = e\n",
        "                    out.append((beg,end,fsent))\n",
        "                    break\n",
        "                sent = sent[len(w):].strip()\n",
        "        return out\n",
        "\n",
        "    def finish(self):\n",
        "        \"\"\"Flush the incomplete text when the whole processing ends.\n",
        "        Returns: the same format as self.process_iter()\n",
        "        \"\"\"\n",
        "        o = self.transcript_buffer.complete()\n",
        "        f = self.to_flush(o)\n",
        "        logger.debug(f\"last, noncommited: {f}\")\n",
        "        self.buffer_time_offset += len(self.audio_buffer)/16000\n",
        "        return f\n",
        "\n",
        "\n",
        "    def to_flush(self, sents, sep=None, offset=0, ):\n",
        "        # concatenates the timestamped words or sentences into one sequence that is flushed in one line\n",
        "        # sents: [(beg1, end1, \"sentence1\"), ...] or [] if empty\n",
        "        # return: (beg1,end-of-last-sentence,\"concatenation of sentences\") or (None, None, \"\") if empty\n",
        "        if sep is None:\n",
        "            sep = self.asr.sep\n",
        "        t = sep.join(s[2] for s in sents)\n",
        "        if len(sents) == 0:\n",
        "            b = None\n",
        "            e = None\n",
        "        else:\n",
        "            b = offset + sents[0][0]\n",
        "            e = offset + sents[-1][1]\n",
        "        return (b,e,t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G0LPTKl-ZXh"
      },
      "outputs": [],
      "source": [
        "class VACOnlineASRProcessor(OnlineASRProcessor):\n",
        "    '''Wraps OnlineASRProcessor with VAC (Voice Activity Controller).\n",
        "\n",
        "    It works the same way as OnlineASRProcessor: it receives chunks of audio (e.g. 0.04 seconds),\n",
        "    it runs VAD and continuously detects whether there is speech or not.\n",
        "    When it detects end of speech (non-voice for 500ms), it makes OnlineASRProcessor to end the utterance immediately.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, online_chunk_size, *a, **kw):\n",
        "        self.online_chunk_size = online_chunk_size\n",
        "\n",
        "        self.online = OnlineASRProcessor(*a, **kw)\n",
        "\n",
        "        # VAC:\n",
        "        # import torch\n",
        "        # model, _ = torch.hub.load(\n",
        "        #     repo_or_dir='snakers4/silero-vad',\n",
        "        #     model='silero_vad'\n",
        "        # )\n",
        "        # from silero_vad_iterator import FixedVADIterator\n",
        "        # self.vac = FixedVADIterator(model)\n",
        "        # we use the default options there: 500ms silence, 100ms padding, etc.\n",
        "\n",
        "        self.logfile = self.online.logfile\n",
        "        self.init()\n",
        "\n",
        "    def init(self):\n",
        "        self.online.init()\n",
        "        #self.vac.reset_states()\n",
        "        self.current_online_chunk_buffer_size = 0\n",
        "\n",
        "        self.is_currently_final = False\n",
        "\n",
        "        self.status = None  # or \"voice\" or \"nonvoice\"\n",
        "        self.audio_buffer = np.array([],dtype=np.float32)\n",
        "        self.buffer_offset = 0  # in frames\n",
        "\n",
        "    def clear_buffer(self):\n",
        "        #self.buffer_offset += len(self.audio_buffer)\n",
        "        self.audio_buffer = np.array([],dtype=np.float32)\n",
        "\n",
        "\n",
        "    def insert_audio_chunk(self, audio, res, buffer_offset_sample):\n",
        "        self.audio_buffer = np.append(self.audio_buffer, audio)\n",
        "        self.buffer_offset = buffer_offset_sample\n",
        "\n",
        "        if res is not None:\n",
        "            #frame = list(res.values())[0]-self.buffer_offset\n",
        "            if res.start_time is not None and res.end_time is None:\n",
        "                frame = res.start_time-self.buffer_offset\n",
        "                print(\"res_start\", res.start_time)\n",
        "                self.status = 'voice'\n",
        "                send_audio = self.audio_buffer[frame:]\n",
        "                self.online.init(offset=(frame+self.buffer_offset)/self.SAMPLING_RATE)\n",
        "                self.online.insert_audio_chunk(send_audio)\n",
        "                self.current_online_chunk_buffer_size += len(send_audio)\n",
        "                self.clear_buffer()\n",
        "            elif res.end_time is not None and res.start_time is None:\n",
        "                frame = res.end_time-self.buffer_offset\n",
        "                print(\"res_end\", res.end_time)\n",
        "                self.status = 'nonvoice'\n",
        "                send_audio = self.audio_buffer[:frame]\n",
        "                self.online.insert_audio_chunk(send_audio)\n",
        "                self.current_online_chunk_buffer_size += len(send_audio)\n",
        "                self.is_currently_final = True\n",
        "                self.clear_buffer()\n",
        "            else:\n",
        "                print(\"res_start\", res.start_time)\n",
        "                print(\"res_end\", res.end_time)\n",
        "                beg = res.start_time-self.buffer_offset\n",
        "                end = res.end_time-self.buffer_offset\n",
        "                self.status = 'nonvoice'\n",
        "                send_audio = self.audio_buffer[beg:end]\n",
        "                self.online.init(offset=(beg+self.buffer_offset)/self.SAMPLING_RATE)\n",
        "                self.online.insert_audio_chunk(send_audio)\n",
        "                self.current_online_chunk_buffer_size += len(send_audio)\n",
        "                self.is_currently_final = True\n",
        "                self.clear_buffer()\n",
        "        else:\n",
        "            if self.status == 'voice':\n",
        "                self.online.insert_audio_chunk(self.audio_buffer)\n",
        "                self.current_online_chunk_buffer_size += len(self.audio_buffer)\n",
        "                self.clear_buffer()\n",
        "\n",
        "\n",
        "    def process_iter(self):\n",
        "        if self.is_currently_final:\n",
        "            return self.finish()\n",
        "        elif self.current_online_chunk_buffer_size > self.SAMPLING_RATE*self.online_chunk_size:\n",
        "            self.current_online_chunk_buffer_size = 0\n",
        "            ret = self.online.process_iter()\n",
        "            return ret\n",
        "        else:\n",
        "            print(\"no online update, only VAD\", self.status, file=self.logfile)\n",
        "            return (None, None, \"\")\n",
        "\n",
        "    def finish(self):\n",
        "        ret = self.online.finish()\n",
        "        self.current_online_chunk_buffer_size = 0\n",
        "        self.is_currently_final = False\n",
        "        return ret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DlMv4vBfW2Ng"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/trungnothot/miniconda3/envs/thesis/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "import os\n",
        "import numpy as np\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "from typing import Optional\n",
        "import uvicorn\n",
        "\n",
        "\n",
        "# Khởi tạo FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Khởi tạo Whisper ASR (giữ cấu hình như bot.py)\n",
        "asr = FasterWhisperASR(modelsize=\"large-v3\", lan=\"en\")\n",
        "min_chunk_size = 1\n",
        "online_asr_proc = VACOnlineASRProcessor(min_chunk_size, asr, tokenizer=None, buffer_trimming=(\"segment\", 15))\n",
        "online_asr_proc.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRlJW8C5W522",
        "outputId": "564718b5-1b9d-4536-80bf-e8b3f565c719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đang warmup Whisper...\n",
            "Warmup hoàn tất!\n"
          ]
        }
      ],
      "source": [
        "# Warmup Whisper\n",
        "warmup_file = \"samples_jfk.wav\"\n",
        "print(\"Đang warmup Whisper...\")\n",
        "if os.path.isfile(warmup_file):\n",
        "    warmup_audio = load_audio_chunk(warmup_file, 0, 1)  # Lấy 1 giây\n",
        "    asr.transcribe(warmup_audio)\n",
        "    print(\"Warmup hoàn tất!\")\n",
        "else:\n",
        "    print(f\"Không tìm thấy {warmup_file}. Vui lòng tải file lên Colab!\")\n",
        "    # Tiếp tục chạy nhưng không có warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1UPTlyoO65X",
        "outputId": "55f71714-0a49-4bc5-caf0-bce42d4232c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL công khai: NgrokTunnel: \"https://9f19-123-20-55-211.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "Lưu ý: Bạn cần cập nhật URL này trong file costum_pycord.py của Discord bot\n",
            "Thay đổi api_url = 'https://url-của-bạn/transcribe'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [6127]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     127.0.0.1:55656 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     127.0.0.1:55656 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     127.0.0.1:41814 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     127.0.0.1:41814 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [6127]\n"
          ]
        }
      ],
      "source": [
        "# Định nghĩa cấu trúc dữ liệu nhận vào\n",
        "class SegmentInfo(BaseModel):\n",
        "    start_time: Optional[int]\n",
        "    end_time: Optional[int]\n",
        "\n",
        "class AudioChunk(BaseModel):\n",
        "    audio: list[float]\n",
        "    ssrc_id: int\n",
        "    segment_infor: Optional[SegmentInfo]\n",
        "    buffer_offset: int\n",
        "\n",
        "# Endpoint nhận và xử lý âm thanh\n",
        "@app.post(\"/transcribe\")\n",
        "async def transcribe_audio(chunk: AudioChunk):\n",
        "    # Chuyển danh sách thành mảng numpy float32\n",
        "    audio = np.array(chunk.audio, dtype=np.float32)\n",
        "\n",
        "\n",
        "    # Xử lý bằng OnlineASRProcessor\n",
        "    online_asr_proc.insert_audio_chunk(audio, chunk.segment_infor, chunk.buffer_offset)\n",
        "    result = online_asr_proc.process_iter()\n",
        "    beg, end, text = result\n",
        "\n",
        "    print(\"dcm_1\", flush = True)\n",
        "    if chunk.segment_infor is not None:\n",
        "      print(\"segmentInfor if not None *************************\", flush = True)\n",
        "\n",
        "\n",
        "    # In transcription lên console Colab\n",
        "    if text:\n",
        "        print(f\"Transcription (SSRC {chunk.ssrc_id}) (beg: {beg}) (end: {end}): {text}\")\n",
        "        print()\n",
        "\n",
        "    return {\"transcription\": text}\n",
        "\n",
        "# Tạo URL công khai bằng ngrok\n",
        "# Lưu ý: URL ngrok sẽ thay đổi mỗi khi bạn chạy lại notebook này\n",
        "# Sau khi có URL mới, bạn cần cập nhật URL này trong file Discord bot (costum_pycord.py)\n",
        "ngrok_token = \"2w5OIFiULufPaz4fJG07MDIk4p1_83v3K2MfZTimUFxvdm3Uk\"  # Thay bằng token ngrok của bạn\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# Bạn có thể sử dụng ngrok.connect với reserved subdomain để có URL cố định hơn\n",
        "# Ví dụ: public_url = ngrok.connect(8000, subdomain=\"mywhisperservice\")\n",
        "# Lưu ý: Tính năng này yêu cầu tài khoản ngrok trả phí\n",
        "\n",
        "tunnel = ngrok.connect(8000)\n",
        "# Get the public URL as a string\n",
        "public_url = str(tunnel.public_url)\n",
        "\n",
        "# Lưu URL vào file\n",
        "with open(\"/home/trungnothot/Study/Thesis/audio-us-discord-bot/discord_recording_bot/ngrok_url.txt\", \"w\") as f:\n",
        "    f.write(public_url)\n",
        "\n",
        "print(f\"URL công khai: {public_url}\")\n",
        "print(\"Lưu ý: Bạn cần cập nhật URL này trong file costum_pycord.py của Discord bot\")\n",
        "print(\"Thay đổi api_url = 'https://url-của-bạn/transcribe'\")\n",
        "\n",
        "# Chạy server\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
